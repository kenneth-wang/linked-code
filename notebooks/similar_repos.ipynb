{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d2eb2f-dd76-46f6-bbf2-bca489b0f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, types, functions\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289fddc7-a9ed-47af-907a-fe3e4bf1c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 22:11:43 WARN Utils: Your hostname, Kenneths-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.1 instead (on interface en0)\n",
      "21/10/31 22:11:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kenneth/Documents/mcomp/CS5344/linked-code/linked_code_venv/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kenneth/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kenneth/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-baae80fb-02de-4335-8bb9-716405cba66c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.3.0 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 671ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.3.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   25  |   0   |   0   |   0   ||   25  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-baae80fb-02de-4335-8bb9-716405cba66c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 25 already retrieved (0kB/26ms)\n",
      "21/10/31 22:11:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/31 22:11:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the MondoDB database\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder \n",
    "    .appName(\"reviews\") \n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/linked_code.repos\") \n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/linked_code.users\") \n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.0\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c290fef-ed3a-46ec-85c2-198f6220ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 22:13:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "21/10/31 22:13:34 WARN MongoInferSchema: Array Field 'starred' contains conflicting types converting to StringType\n"
     ]
    }
   ],
   "source": [
    "# Connect to the repos and user collections\n",
    "repos = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "# repos = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://localhost:27017/cs5344.repos\").load()\n",
    "users = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://localhost:27017/linked_code.users\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5be098-7ef4-431d-b0db-a531a3abb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only a few columns\n",
    "cols_to_keep = ['id','name', 'description', 'language']\n",
    "df = repos.select(cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6005c9b-d8f6-4232-b059-534673382c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only these columns for users\n",
    "df_users = users.select(['id','starred'])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc1c5d8-4a32-4773-9c2f-7fc3193144a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out repos with no name or description\n",
    "null_values = ['nan','NA','null']\n",
    "\n",
    "df_filtered = ( \n",
    "    df\n",
    "    .filter(~df['description'].isNull())\n",
    "    .filter(~df['description'].isin(null_values))\n",
    "    .filter(~df['name'].isNull())\n",
    "    .filter(~df['name'].isin(null_values))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf138b-dac5-4bf6-95b4-6940c33f3567",
   "metadata": {},
   "source": [
    "## Compute similarity scores for repo descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74c574",
   "metadata": {},
   "source": [
    "#### Set up the SparkNLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eade59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec19eac-c216-47bd-8e3c-acc0a834443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tfhub_use\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13944347-5439-4918-aa84-94bfdc091f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepoQueryPipeline():\n",
    "    def __init__(self, model_name):\n",
    "        # Transforms the input text into a document usable by the SparkNLP pipeline.\n",
    "        self.document_assembler = DocumentAssembler()\n",
    "        self.document_assembler.setInputCol('text')\n",
    "        self.document_assembler.setOutputCol('document')\n",
    "\n",
    "        # Separates the text into individual tokens (words and punctuation).\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.setInputCols(['document'])\n",
    "        self.tokenizer.setOutputCol('token')\n",
    "        \n",
    "        # Encodes the text as a single vector representing semantic features.\n",
    "        self.sentence_encoder = UniversalSentenceEncoder.pretrained(name=model_name)\n",
    "        self.sentence_encoder.setInputCols(['document', 'token'])\n",
    "        self.sentence_encoder.setOutputCol('sentence_embeddings')\n",
    "        \n",
    "    def init_pipeline(self):\n",
    "        self.nlp_pipeline = Pipeline(stages=[\n",
    "            self.document_assembler, \n",
    "            self.tokenizer,\n",
    "            self.sentence_encoder\n",
    "        ])\n",
    "        \n",
    "        # Fit the model to an empty data frame so it can be used on inputs.\n",
    "        empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "        pipeline_model = self.nlp_pipeline.fit(empty_df)\n",
    "        self.light_pipeline = LightPipeline(pipeline_model)\n",
    "        \n",
    "    def get_similarity(self, emb_matrix):\n",
    "        return np.matmul(emb_matrix, emb_matrix.transpose())\n",
    "    \n",
    "    def _encode_df(self, df):\n",
    "        encoded_df = self.light_pipeline.transform(df)\n",
    "            \n",
    "        return encoded_df\n",
    "\n",
    "    def convert_query_repo_desc_to_df(self, query_repo_desc):\n",
    "        query_formatted = [(1, query_repo_desc)]\n",
    "\n",
    "        columns = [\"query_num\", \"text\"]\n",
    "        query_df = spark.createDataFrame(data=query_formatted, schema=columns).select(\"text\")\n",
    "        \n",
    "        return query_df\n",
    "    \n",
    "    def _extract_emb_matrix(self, encoded_df):\n",
    "        embs = []\n",
    "        for r in encoded_df.collect():\n",
    "            embs.append(r.sentence_embeddings[0].embeddings)\n",
    "        emb_matrix = np.array(embs)\n",
    "        \n",
    "        return emb_matrix\n",
    "    \n",
    "    def get_emb_matrix(self, df):\n",
    "        # Rename the column to match the pipeline model\n",
    "        df = df.withColumnRenamed(df.columns[0],'text')\n",
    "\n",
    "        encoded_df = self._encode_df(df)\n",
    "        emb_matrix = self._extract_emb_matrix(encoded_df)\n",
    "        return emb_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb660119",
   "metadata": {},
   "source": [
    "#### Initialize the SparkNLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c869e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 22:13:54 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "21/10/31 22:13:54 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ â€” ]Download done! Loading the resource.\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 22:14:18.679690: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "repo_q_pl = RepoQueryPipeline(MODEL_NAME)\n",
    "repo_q_pl.init_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521ba5",
   "metadata": {},
   "source": [
    "#### Select the repository size that you want to query against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e0901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acebe146",
   "metadata": {},
   "source": [
    "#### Encode the repositories' description text to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be183c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_filtered.limit(REPOSITORY_SIZE).select('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5920cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "embs_matrix = repo_q_pl.get_emb_matrix(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ac3b1",
   "metadata": {},
   "source": [
    "#### Insert query repo description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a279579",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_q_desc = \"Python machine learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82846ff9",
   "metadata": {},
   "source": [
    "#### Encode the query repo description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e98a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "repo_q_desc_df = repo_q_pl.convert_query_repo_desc_to_df(repo_q_desc)\n",
    "repo_q_desc_emb_matrix = repo_q_pl.get_emb_matrix(repo_q_desc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4182bd6",
   "metadata": {},
   "source": [
    "#### Initialize the Annoy nearest neighbours pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "class AnnoyIdx():\n",
    "    def __init__(self, embedding_size, dist_measure):\n",
    "        self.t = AnnoyIndex(embedding_size, dist_measure)\n",
    "\n",
    "    def build(self, embs_matrix):\n",
    "        for i, emb in enumerate(embs_matrix):\n",
    "            self.t.add_item(i, emb)\n",
    "        self.t.build(10)\n",
    "\n",
    "    def query(self, query_embedding, num_nbrs, inc_dist=False):\n",
    "        return self.t.get_nns_by_vector(query_embedding, num_nbrs, include_distances=inc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606cd9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_idx = AnnoyIdx(512, \"angular\")\n",
    "annoy_idx.build(embs_matrix)\n",
    "result = annoy_idx.query(repo_q_desc_emb_matrix[0], 3, inc_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c32ac",
   "metadata": {},
   "source": [
    "#### Retrieve the most relevant repositories given the input query repository description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de86d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_desc_name = {i: [row[\"description\"], row[\"name\"]] for i, row in enumerate(df_filtered.limit(REPOSITORY_SIZE).collect())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c2110dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant repos are:\n",
      "0\n",
      "name: dumbo\n",
      "description: Python module that allows one to easily write and run Hadoop programs.\n",
      "distance: 0.614687979221344\n",
      "\n",
      "\n",
      "1\n",
      "name: ocaml-clustering\n",
      "description: Collection of clustering algorithms written in Ocaml\n",
      "distance: 0.6442485451698303\n",
      "\n",
      "\n",
      "2\n",
      "name: crapvine\n",
      "description: A python implementation of Grapevine\n",
      "distance: 0.6676327586174011\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Relevant repos are:\")\n",
    "for i, idx in enumerate(result[0]):\n",
    "    print(i)\n",
    "    print(f\"name: {id_to_desc_name[idx][1]}\")\n",
    "    print(f\"description: {id_to_desc_name[idx][0]}\")\n",
    "    print(f\"distance: {result[1][i]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec25fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3840f4b3f07f540e315edc48be419897380cf55b0e8ac4e5d54833b15ccd80ab"
  },
  "kernelspec": {
   "display_name": "linked_code_venv",
   "language": "python",
   "name": "linked_code_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
